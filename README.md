# MCTS
### Monte-Carlo Tree Search for use on gym environments. 

Implements the popular [UCT](http://ggp.stanford.edu/readings/uct.pdf) algorithm in the reinforcement learning setting. This algorithm is useful when a simulator of the environment is available. Starting with the current state, all actions are tried in a simulation. If the state-action pair has been encountered before, UCT picks the action with the highest upper-confidence bound on Q-value. This is implemented in the function `select_child`. The first time a particular state-action pair is encountered (a leaf node), its Q-value is estimated by randomly picking actions until the simulation ends. This estimated Q-value for the leaf node is incorporated into the Q-values of transitions leading to the leaf node, with appropriate discounting, by the `backpropagate` function.
